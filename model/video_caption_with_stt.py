import torch
from transformers import (
    AutoProcessor, 
    AutoModelForVideoClassification,
    pipeline
)
import whisper
import cv2
import os
from moviepy.editor import VideoFileClip
import numpy as np
from tqdm import tqdm
from pathlib import Path
import json

# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
BASE_PATH = '/home/aikusrv02/aiku/video_retrieval'
SHORTS_PATH = f"{BASE_PATH}/data/home_alone/shorts_trimmed"
OUTPUT_PATH = f"{BASE_PATH}/data/home_alone/shorts_metadata"

class MultiModalAnalyzer:
    def __init__(self):
        print("ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        try:
            # ë¹„ë””ì˜¤ ë¶„ì„ ëª¨ë¸
            self.video_processor = AutoProcessor.from_pretrained("MCG-NJU/videomae-base-finetuned-kinetics")
            self.video_model = AutoModelForVideoClassification.from_pretrained("MCG-NJU/videomae-base-finetuned-kinetics")
            
            # STT ëª¨ë¸ (Whisper)
            self.stt_model = whisper.load_model("base")
            
            # ìº¡ì…”ë‹ íŒŒì´í”„ë¼ì¸
            self.caption_pipeline = pipeline(
                "video-classification", 
                model="facebook/timesformer-base-finetuned-k400",
                device=0 if torch.cuda.is_available() else -1
            )
            
            # GPU ì„¤ì •
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.video_model.to(self.device)
            
            print(f"ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ. ì‚¬ìš© ì¥ì¹˜: {self.device}")
            
        except Exception as e:
            print(f"ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def extract_video_features(self, video_path: str) -> list:
        """ë¹„ë””ì˜¤ í”„ë ˆì„ ë¶„ì„"""
        try:
            frames = []
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            interval = max(total_frames // 16, 1)
            
            for i in range(16):
                frame_idx = min(i * interval, total_frames - 1)
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    frame = cv2.resize(frame, (224, 224))
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frames.append(frame)
            
            cap.release()
            
            if not frames:
                return []
            
            # ë¹„ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ
            inputs = self.video_processor(list(frames), return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.video_model(**inputs)
                video_probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
                _, indices = torch.topk(video_probs, k=5)
                
            return [self.video_model.config.id2label[idx.item()] for idx in indices[0]]
            
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def extract_stt(self, video_path: str) -> dict:
        """ë¹„ë””ì˜¤ì—ì„œ ìŒì„± ì¶”ì¶œ ë° STT ìˆ˜í–‰"""
        try:
            # ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            temp_audio = "temp_audio.wav"
            
            # ì˜¤ë””ì˜¤ ì¶”ì¶œ
            video = VideoFileClip(video_path)
            if video.audio is None:
                print("ì˜¤ë””ì˜¤ íŠ¸ë™ì´ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ì˜¤ë””ì˜¤ ì €ì¥
            video.audio.write_audiofile(temp_audio, verbose=False, logger=None)
            video.close()
            
            # STT ìˆ˜í–‰
            result = self.stt_model.transcribe(temp_audio)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.remove(temp_audio)
            
            return result
            
        except Exception as e:
            print(f"STT ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def generate_caption(self, video_features: list, stt_result: dict) -> str:
        """ë¹„ë””ì˜¤ íŠ¹ì§•ê³¼ STT ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ìº¡ì…˜ ìƒì„±"""
        try:
            caption_elements = []
            
            # 1. ë¹„ë””ì˜¤ í–‰ë™/ì¥ë©´ ì„¤ëª… ì¶”ê°€
            if video_features:
                main_action = video_features[0]
                caption_elements.append(f"ì˜ìƒì—ì„œ '{main_action}' í–‰ë™ì´ ê°ì§€ë˜ì—ˆìœ¼ë©°")
            
            # 2. STT í…ìŠ¤íŠ¸ ì¶”ê°€
            if stt_result and 'text' in stt_result and stt_result['text'].strip():
                caption_elements.append(f"ë‹¤ìŒê³¼ ê°™ì€ ëŒ€ì‚¬ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: '{stt_result['text'].strip()}'")
            
            # 3. ìº¡ì…˜ ì¡°í•©
            if caption_elements:
                return " ".join(caption_elements)
            else:
                return "ìº¡ì…˜ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"ìº¡ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return "ìº¡ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def generate_shorts_title(self, video_features: list, stt_result: dict, caption: str) -> dict:
        """ìº¡ì…˜ì„ ë°”íƒ•ìœ¼ë¡œ ì‡¼ì¸  ìŠ¤íƒ€ì¼ì˜ ì œëª© ìƒì„±"""
        
        # ê°ì •/ìƒí™©ë³„ ì´ëª¨ì§€ ë§¤í•‘
        emotion_emoji = {
            'funny': ['ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜†'],
            'exciting': ['ğŸ”¥', 'âš¡', 'ğŸ’¥'],
            'surprising': ['ğŸ˜±', 'ğŸ˜²', 'ğŸ¤¯'],
            'cute': ['ğŸ˜Š', 'ğŸ¥°', 'ğŸ’–'],
            'cool': ['ğŸ˜', 'ğŸ†’', 'âœ¨'],
            'scary': ['ğŸ˜¨', 'ğŸ™€', 'ğŸ‘»'],
            'action': ['ğŸ’ª', 'ğŸƒ', 'ğŸ‘Š'],
            'clever': ['ğŸ§ ', 'ğŸ’¡', 'ğŸ¤“']
        }
        
        # í–‰ë™/í‚¤ì›Œë“œë³„ ê°ì • ë§¤í•‘
        action_emotion = {
            'running': 'action',
            'jumping': 'exciting',
            'falling': 'funny',
            'laughing': 'funny',
            'fighting': 'action',
            'dancing': 'cool',
            'screaming': 'surprising',
            'hiding': 'scary',
            'planning': 'clever'
        }
        
        try:
            # 1. ì£¼ìš” ê°ì •/ìƒí™© íŒŒì•…
            emotion = 'exciting'  # ê¸°ë³¸ê°’
            
            # ë¹„ë””ì˜¤ íŠ¹ì§•ì—ì„œ ê°ì • íŒŒì•…
            if video_features:
                main_action = video_features[0].lower()
                for action, emo in action_emotion.items():
                    if action in main_action:
                        emotion = emo
                        break
            
            # STT í…ìŠ¤íŠ¸ì—ì„œ ê°ì • íŒŒì•…
            if stt_result and 'text' in stt_result:
                text = stt_result['text'].lower()
                for action, emo in action_emotion.items():
                    if action in text:
                        emotion = emo
                        break
            
            # 2. ì´ëª¨ì§€ ì„ íƒ
            emoji = np.random.choice(emotion_emoji[emotion])
            
            # 3. ì œëª© í…œí”Œë¦¿
            templates = {
                'funny': [
                    f"ì¼€ë¹ˆì´ ë˜ ëŒ€ì‘ì „ ì„±ê³µí–ˆìŠµë‹ˆë‹¤ {emoji}",
                    f"ì´ê²Œ ë˜ë„¤?ã…‹ã…‹ã…‹ {emoji}",
                    f"ë„ë‘‘ë“¤ ë¹µí„°ì§„ ìˆœê°„ {emoji}"
                ],
                'exciting': [
                    f"ì¼€ë¹ˆ ë ˆì „ë“œ ìˆœê°„ {emoji}",
                    f"ì´ê±¸ ìƒê°í•´ë‚´ë‹¤ë‹ˆ.. {emoji}",
                    f"ë„ë‘‘ì¡ê¸° ëŒ€ì‘ì „ {emoji}"
                ],
                'surprising': [
                    f"ìƒìƒë„ ëª»í•œ ë°˜ì „ {emoji}",
                    f"ë„ë‘‘ë“¤ ê²½ì•…í•œ ìˆœê°„ {emoji}",
                    f"ì¼€ë¹ˆì˜ ë†€ë¼ìš´ ì‘ì „ {emoji}"
                ],
                'clever': [
                    f"ì²œì¬ ì¼€ë¹ˆì˜ ìˆ˜í•™ {emoji}",
                    f"ì´ ê¼¬ë§ˆ ë­”ê°€ ë‹¤ë¥´ë‹¤ {emoji}",
                    f"ì¼€ë¹ˆì˜ ë˜‘ë˜‘í•œ í•¨ì • {emoji}"
                ]
            }
            
            # 4. ì œëª© ìƒì„±
            title_templates = templates.get(emotion, templates['exciting'])
            main_title = np.random.choice(title_templates)
            
            # 5. í•´ì‹œíƒœê·¸ ì¶”ê°€ (30% í™•ë¥ )
            hashtags = ["#í™ˆì–¼ë¡ ", "#ë‚˜í™€ë¡œì§‘ì—", "#ì¼€ë¹ˆ", "#ë ˆì „ë“œ"]
            if np.random.random() < 0.3:
                main_title += f" {np.random.choice(hashtags)}"
            
            # 6. ì¸ë„¤ì¼ìš© ê°„ë‹¨ í…ìŠ¤íŠ¸ ìƒì„±
            thumbnail_text = main_title.split('#')[0].strip()  # í•´ì‹œíƒœê·¸ ì œì™¸
            
            return {
                'title': main_title,
                'thumbnail_text': thumbnail_text,
                'emotion': emotion,
                'emoji_used': emoji
            }
            
        except Exception as e:
            print(f"ì œëª© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                'title': f"ì¼€ë¹ˆì˜ ë ˆì „ë“œ ìˆœê°„ âœ¨",
                'thumbnail_text': "ì¼€ë¹ˆì˜ ë ˆì „ë“œ ìˆœê°„",
                'emotion': 'exciting',
                'emoji_used': 'âœ¨'
            }

def process_video(video_path: str, analyzer: MultiModalAnalyzer) -> dict:
    """ë‹¨ì¼ ë¹„ë””ì˜¤ ì²˜ë¦¬"""
    try:
        print(f"\në¹„ë””ì˜¤ ë¶„ì„ ì¤‘: {Path(video_path).name}")
        
        results = {
            'video_path': video_path,
            'video_features': [],
            'stt_result': None,
            'caption': ''
        }
        
        # 1. ë¹„ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ
        print("ë¹„ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        results['video_features'] = analyzer.extract_video_features(video_path)
        
        # 2. STT ìˆ˜í–‰
        print("ìŒì„± ì¸ì‹ ìˆ˜í–‰ ì¤‘...")
        results['stt_result'] = analyzer.extract_stt(video_path)
        
        # 3. ìº¡ì…˜ ìƒì„±
        print("ìº¡ì…˜ ìƒì„± ì¤‘...")
        results['caption'] = analyzer.generate_caption(
            results['video_features'],
            results['stt_result']
        )
        
        # 4. ì‡¼ì¸  ìŠ¤íƒ€ì¼ì˜ ì œëª© ìƒì„±
        print("ì œëª© ìƒì„± ì¤‘...")
        results['shorts_title'] = analyzer.generate_shorts_title(
            results['video_features'],
            results['stt_result'],
            results['caption']
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n=== ë¶„ì„ ê²°ê³¼ ===")
        print("\në¹„ë””ì˜¤ íŠ¹ì§•:")
        for feat in results['video_features']:
            print(f"- {feat}")
            
        if results['stt_result']:
            print("\nSTT ê²°ê³¼:")
            print(results['stt_result']['text'])
            
        print("\nìƒì„±ëœ ìº¡ì…˜:")
        print(results['caption'])
        
        print("\nì œëª© ìƒì„± ê²°ê³¼:")
        print(results['shorts_title'])
        
        return results
        
    except Exception as e:
        print(f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def save_results(results: dict, output_dir: str):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        os.makedirs(output_dir, exist_ok=True)
        
        output_path = os.path.join(
            output_dir,
            f"{Path(results['video_path']).stem}_analysis.json"
        )
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
            
        print(f"\nê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        
    except Exception as e:
        print(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n=== ë©€í‹°ëª¨ë‹¬ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘ ===\n")
    
    # ë¹„ë””ì˜¤ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    if not os.path.exists(SHORTS_PATH):
        print(f"ì˜¤ë¥˜: ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SHORTS_PATH}")
        return
        
    video_files = [f for f in os.listdir(SHORTS_PATH) if f.endswith(('.mp4', '.avi', '.mov'))]
    
    if not video_files:
        print(f"ì˜¤ë¥˜: ì…ë ¥ ë””ë ‰í† ë¦¬ì— ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {SHORTS_PATH}")
        return
    
    print(f"ì´ {len(video_files)}ê°œì˜ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    
    try:
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = MultiModalAnalyzer()
        
        # ê° ë¹„ë””ì˜¤ ì²˜ë¦¬
        for file in tqdm(video_files, desc="ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘"):
            video_path = os.path.join(SHORTS_PATH, file)
            results = process_video(video_path, analyzer)
            
            if results:
                save_results(results, OUTPUT_PATH)
            print("-" * 50)
        
        print("\nëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main() 